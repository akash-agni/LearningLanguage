# Read The Paper
Reading papers is one of the most important habits to fall into if one wishes to be a successful Machine Learning Engineer, an advice I have ignored for a long time.
But its never too late, I have picked the habit of auditing as many papers as I can, but once in a while someone puts out work which redefines the landscape and through this repo my intention is to implement such milestone papers through `python`.

* 1 -[Attention is All You Need](https://github.com/akash-agni/ReadThePaper/blob/main/Attention%20Is%20All%20You%20Need.ipynb)[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/akash-agni/LearningLanguage/blob/main/Attention%20Is%20All%20You%20Need.ipynb)

    <i>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin</i>

    This paper introduced the concept of <b>Transformers</b> which has since been the leading architecture for most Sequence-to-Sequence model like <b>BERT</b> and <b>GPT-2</b>.

    Before this RNN based model were dominent, but the big problem with them was they were unable to solve long term dependecies while training, this limited their usage over long sequences.

    The notebook is inspired from <a href="https://github.com/bentrevett/pytorch-seq2seq">here</a> for more detailed turorial on earlier language models check out their work.

    2 - Deep Convolutional Generative Adversial Network.

## References
- https://github.com/bentrevett/pytorch-seq2seq
- https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/
